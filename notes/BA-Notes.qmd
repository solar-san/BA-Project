---
title: "Business Analytics"
subtitle: "Notes"
author: "solar-san"
date-modified: "`r Sys.Date()`"
format:
  html:
    theme: github
    toc: true
    toc-location: left
    fig-align: center
    fig-width: 10
    fig-height: 8
    html-math-method: katex
    code-overflow: scroll
    code-copy: hover
    code-fold: show
    highlight-style: breeze
    reference-location: margin
    citation-location: margin
    tbl-cap-location: margin
    fig-cap-location: margin
    citations-hover: true
    footnotes-hover: true
    header-includes: |
      <meta name="author" content="solar-san">
      <meta name="image" property="">
      <meta 
      name="description"
      content="">
      <link rel="preconnect" href="https://fonts.googleapis.com">
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&family=Fira+Code&display=swap" rel="stylesheet">
mainfont: "Atkinson Hyperlegible"
monofont: 'Fira Code'
---

![](https://raw.githubusercontent.com/solar-san/BA-Project/main/docs/figures/BA-Project_header.png)

```{r setup}
#| echo: false
#| results: hide
#| warning: false
knitr::opts_chunk$set(
  echo = T
)

lib_list <- list(
  "tidyverse",
  "fpp3",
  "ggthemes",
  "patchwork"
)

lapply(
  lib_list,
  require,
  quietly = TRUE, 
  warn.conflicts = FALSE, 
  character.only = TRUE
)

theme_set(
  theme_tufte(
    base_size = 20,
    base_family = "Atkinson Hyperlegible"
  )
)

theme_update(
    legend.position = "top"
)
```

# Introduction

> What is _Business Analytics_? It is an applied discipline, tasked with gaining insight about business operations and make better, fact-based decisions: a basic pillar of being _data-driven_. It combines Statistics, Computer Sciences, Management Science, Operational Research, and much more.

It has three main components:

1. __Descriptive analytics__: providing insight into business questions by summarizing and visualizing observed data. Typically based on simple summaries and charts^[Any predictive or prescriptive model is based on some steps of _descriptive analytics_.].
2. __Predictive analytics__: providing insight into the possible future by extrapolating patterns found in the historical data. Typically making use of statistical or machine learning predictive models (remember the Statistical Learning and Data Analytic courses).
3. __Prescriptive analytics__: providing ways to make the best possible decisions based on the available information.

> __Syllabus__:

- Time series data.
- Quantile regression.
- Simulation and Monte Carlo analysis.

# Forecasting and Time Series

Predictions have been a human interest throghout history. The ability to forecast depends on:

- How much we understand the factors at play, whether our model of the phenomenon is good or not.
- How much data is available.
- How similar the future will be to the past.
- The volatility of the process of interest.
- Whether the forecasts can affect the thing we are trying to forecast.

:::{.callout-caution}
_Data-based forecasts_ should be based on _relevant_ data of the _highest possible quality_.
:::

However, data are not always available.

- Qualitative forecasting is used when no data is available.
- Quantitative forecasting is used when relevant numerical data is available and is representative of some aspect of the past.

> _Quantitative_ methods _must be appropriate for the problem and for the data used to solve it_. Specific methods are used for _time series_ data.

In time series, _time is a defining property of the data_. While it is a continuous variable, sampling usually occurs at regularly spaced intervals, giving regular time-steps that represent the _period_ of the data.

```{r TS example}
gafa_stock %>% 
  filter(
    Symbol == "GOOG",
    year(Date) == 2018
  ) %>% 
  autoplot(
    Close
  ) +
  labs(
    title = "Google Stock Price at Close",
    x = "Month",
    y = "Value at Close"
  )
```

## Forecasting: the Statistical Perspective

The phenomenon that we want to forecast can be conceptualized as a _stochastic process_, which is a _succession of stochastic variables_ $\{ y_t \}$. If we consider its value at time $T$, given $\mathcal{I}$ the set of all available observations for $t = 1, 2, …, T -1$, we can express the _forecast distribution_ as a _joint distribution_, conditional on the available information: $y_t \vert \mathcal{I}$. This notation means: "_the random variable $y_t$, given all that we know in $\mathcal{I}$".

- The _point forecast_ will be the _mean_ or the _median_ of $y_t \vert \mathcal{I}$.
- The _forecast variance_ is $V(y_t \vert \mathcal{I})$. 
- A prediction interval or _interval forecast_ is a range of $y_t$ values with high probability.

When we are dealing with _time series_, we have:

- $y_{t \vert t-1} = y_{t} \vert \{ y_1, y_2, …, y_{t-1} \}$.

A $h-step$ forecast that takes into account all observations up to time $T$ is espressed as:

$$
\hat y_{T + h \vert T} = \mathbb{E} (y_{T + h}\vert y_1, …, y_T)
$$ {#eq-hfor}


Models are useful to forecast, because they account for _uncertainty_.

## Patterns

We can filter three different components of time series. These are _patterns_ that can be used to forecast more effectively, if identified correctly.

1. __Trend__: long-term increase or decrease in the data.
2. __Seasonal__: a series is influenced by a _seasonal factor_ at a _fixed period_ (quarterly, monthly, weekly, daily, hourly).
3. __Cyclic__: data exibit rises and falls that _are not of fixed period_.

:::{.callout-important}
Cyclic patterns have a duration of at least 2 years. 
Seasonal patterns cannot have a periodicity longer that exceed the year and are of _constant length_.
The magnitude of a cycle is often more variable than the magnitude of a seasonal pattern.
:::

The main implications of the differences between seasonal and cyclic patterns is that the timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data.

```{r TS example seasonality}
#| fig-cap: "We can clearly observe a quarterly season pattern in the Australian beer production."
#| fig-height: 6

aus_production %>% 
  filter(
    year(Quarter) >= 2000
  ) %>% 
  autoplot(Beer) +
  labs(
    title = "Australian beer production"
  )
```

```{r TS example ciclyc + seasonal}
us_employment %>%
     filter(
        Title == "Retail Trade", 
        year(Month) >= 1980) %>%
    autoplot(
    Employed
    ) +
    geom_rangeframe() %>%
    labs(
    title = "US employment from 1980"
  )
```

## Autocorrelation

While in standard _regression models_ we assume _independence_ among the observations, this is not true with time series models. We can visualize this by comparing $y_t$ atainst $y_{t-k}$ for different values of $k$. $k$ is the _lag_ vaue, while $y_{t-k}$ the _lagged value_: we define _autocorrelation_ the linear dependence of a variable with its lagged values^[In other words: we are concerned with the dependency of a phenomenon with its own past.].

```{r TS example lag plot}
#| fig-width: 12
#| fig-height: 12
aus_production %>% 
  gg_lag(
    Beer,
    geom = "point"
  ) +
  labs(
    title = "Lag plot for Australian beer production"
  ) +
  theme(
    axis.text.x = element_text(angle = 270),
  )
```

From the following plot we can clearly see that at each 4th lag^[$k = 4$, or $t-4$] there is a clear evidence of correlation between $y_t$ and $y_{t-k}$. This is a tell-tale sign of _seasonality_. 

However, not always we can spot it using visualization techniques:

```{r TS example lag and ciclycality}
#| fig-width: 12
#| fig-height: 12
us_employment %>% 
  filter(
    year(Month) >= 2010,
    Series_ID == "CEU0500000001"
  ) %>%
  gg_lag(
    Employed,
    geom = "point"
  ) +
  labs(
    title = "Lag plot for Australian beer production"
  ) +
  theme(
    axis.text.x = element_text(angle = 270),
  )
```

:::{.callout-caution}
When _multiple patterns_ overlap, we cannot use this lag-charts to spot _seasonality_.
:::

## Autovariance and Autocorrelation

> _Autocovariance_ and _autocorrelation_ measure linear relations between lagged values of a time series $y$.

- $c_k$: sample autocovariane at lag $k$.
- $r_k$: sample autocorrelation at lag $k$^[$r_k$ is almost the same as the _sample correlation_ between $y_t$ and $y_{t−k}$.].

$$
c_k = \frac{1}{T} \sum_{t = k + 1}^T (y_t - \bar y)(y_{t - k} - \bar y)
$${#eq-ck}

$$
r_k = \frac{c_k}{c_0}
$${#eq-rk}

We can visualize the autocorrelations with a _correlogram_ plot^[The correlation for $k=0$ is 0, therefore it is often omitted.]:

```{r ACF TS example 1}
aus_production %>% 
filter(
    year(Quarter) >= 2000
  ) %>%
    ACF(
        Beer,
        lag_max = 8
    ) %>%
    autoplot()
```

```{r ACF TS example 2}
us_employment %>%
    filter(
        Title == "Retail Trade", 
        year(Month) >= 1980
        ) %>%
        ACF(
            Employed,
            lag_max = 32
        ) %>%
        autoplot()
```

- When data have a trend, the autocorrelations for small lags tend to be large and positive.
- When data are seasonal, the autocorrelation will be larger at the seasonal lags^[This is visible at multiples of the seasonal frequency.].
- When data are trended and seasonal, you see a combination of these effects.

> Time series that show no autocorrelation are called _white noise_.

This is a benchmark: if a series have small $r_k$, we can assume that behaves like white noise. The sampling distribution for white noise data is (asymptotically) $N(0, 1/T)$, therefore we can build _confidence intervals_, usually at $\alpha=5%$.

```{r WN plot}
tsibble(
    sample = 1:150, 
    wn = rnorm(150), 
    index = sample
    ) %>%
    autoplot(wn) + 
    labs(
        title = "White noise", 
        y = ""
    )
```


```{r WN ACF plot}
tsibble(
    sample = 1:150, 
    wn = rnorm(150), 
    index = sample
    ) %>%
    ACF() %>%
    autoplot()
```

We can see that even if there is a slight evidence of correlation, the confidence intervals help in choosing to label the series as white noise: it is not statistically significant.

## Transformations

> _Transformations_ have many uses: adjusting the series helps with decomposition and comparisons; also, stabilizing the variance simplifies subsequent analysis.

We have four main classes of transformations, dealing with economic data:

1. Calendar adjustments.
2. Population adjustments.
3. Inflation adjustments.
4. Mathematical transformations.

```{r transformation example}
#| fig-heigth: 12
p1 <- global_economy %>% 
  filter(
    Country == "Australia" |
    Country == "Italy"
    ) %>% 
  autoplot(GDP) +
  labs(
    title= "GDP",
    y = "$US"
    ) +
  theme(
    legend.position = "none"
  )
p2 <- global_economy %>% 
 filter(
    Country == "Australia" |
    Country == "Italy"
    ) %>% 
  autoplot(Population) +
  labs(
    title= "Population",
    y = "$US"
    ) +
  theme(
    legend.position = "none"
  )
p3 <- global_economy %>% 
  filter(
    Country == "Australia" |
    Country == "Italy"
    ) %>% 
  autoplot(GDP/Population) +
  labs(
    title= "GDP per capita", 
    y = "$US"
    )

(p1 + p2) / p3 + plot_annotation(
  title = "Transformation example: from GDP to GDP per capita"
)
```

Mathematical transformations are useful to stabilize the variability of the time series, which should be constant: when it varies with the level of the series, a `log` transformation can eliminate the issue^[For the transformed data.], Changes in a log value are relative changes on the original scale.

$$
Y_t = b_t X_t \to log(Y_t) = log(b_t) * log(X_t)
$${#eq-logtransform}

```{r}
#| fig-heigth: 12
p1 <- aus_production %>% 
  autoplot(Gas)
p2 <- aus_production %>% 
  autoplot(
    log(
      Gas
      )
    ) +
  labs(
    y = expression(log(Gas))
  )

p1 / p2 + plot_annotation(
  title = "Log transformations"
)
```

:::{.callout-tip}
Choosing an interpretable transformation is a great advantage. Simple transformations go a long way.
:::

## Decomposition

