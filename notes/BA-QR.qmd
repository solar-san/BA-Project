---
title: "Quantile Regression"
author: "solar-san"
date-modified: "`r Sys.Date()`"
format:
  html:
    theme: github
    toc: true
    toc-location: right
    margin-header: "BA-Project_header.png"
    fig-align: center
    fig-width: 10
    fig-height: 8
    html-math-method: katex
    code-overflow: scroll
    code-copy: hover
    code-fold: show
    highlight-style: breeze
    tbl-cap-location: top
    fig-cap-location: top
    citations-hover: true
    footnotes-hover: true
    header-includes: |
      <meta name="author" content="solar-san">
      <meta name="image" property="https://raw.githubusercontent.com/solar-san/BA-Project/main/docs/figures/BA-Project_header.png">
      <meta 
      name="description"
      content="Notes on quantile regression for business analysts. It contains a self-contained introduction tje topic, with R code snippets and the necessary mathematical and statistical notation.">
      <link rel="preconnect" href="https://fonts.googleapis.com">
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&family=Fira+Code&display=swap" rel="stylesheet">
mainfont: "Atkinson Hyperlegible"
monofont: 'Fira Code'
---

![](https://raw.githubusercontent.com/solar-san/BA-Project/main/docs/figures/BA-Project_header.png)

```{r setup}
#| echo: false
#| results: hide
#| warning: false
knitr::opts_chunk$set(
  echo = T
)

  
pathmaker <- function(filename, data_path = "/Users/themagician/Documents/DABS/Appunti/Business_Analytics/Lab3/") {
  file_path <- paste0(
    data_path,
    filename
  )
  return(
    file_path
  )
}


lib_list <- list(
  "tidyverse",
  "quantreg",
  "ggthemes",
  "patchwork"
)

lapply(
  lib_list,
  require,
  quietly = TRUE, 
  warn.conflicts = FALSE, 
  character.only = TRUE
)

theme_set(
  theme_tufte(
    base_size = 20,
    base_family = "Atkinson Hyperlegible"
  )
)

theme_update(
    legend.position = "top"
)
```

# Introduction

> What is _Business Analytics_? It is an applied discipline, tasked with gaining insight into business operations and helping to make better, fact-based decisions: a basic pillar of being _data-driven_. It combines Statistics, Computer Sciences, Management Science, Operational Research, and much more.

It has three main components:

1. __Descriptive analytics__: providing insight into business questions by summarizing and visualizing observed data. Typically based on simple summaries and charts^[Any predictive or prescriptive model is based on some steps of _descriptive analytics_.].
2. __Predictive analytics__: providing insight into the possible future by extrapolating patterns found in the historical data. Typically making use of statistical or machine learning predictive models (remember the Statistical Learning and Data Analytic courses).
3. __Prescriptive analytics__: providing ways to make the best possible decisions based on the available information.

> What is the role of Quantile Regression in Business Analytics?

The Ordinary-Least-Squares (OLS) based linear regression is a well-known approach when it comes to model relationships between variables, such as:

$$
Y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{p i + \epsilon_i}
$${#eq-multlinreg}

There are fundamental assumptions behind this model:

- Independence of observations and error terms.
- The error terms are identically distributed.
- We have _homoskedasticity_ and _zero mean_^[This last one, however, is not strictly necessary.].
- Normality of errors, as in $\epsilon \stackrel{iid} {\sim} N(0, \sigma^2_{\epsilon})$^[This assumption allows for inference on the model parameters.].

The main focus is to estimate the _mean_ of the distribution: all the assumptions lead to the following formula.

$$
\mathbb{E} [Y_i \vert X = x_i] = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p
$${#eq-condlinreg}

> Quantile regression allows to deal with data that cannot be described with a _normal distribution_ and that can be summarised effectively using the _mean_^[Warning: the mean is _not_ a robust indicator of position.].

The main consequence of having homoskedastic and normal errors are:

- Symmetry.
- If the mean and scaled are known, we can assume that we know the whole distribution. Therefore, knowing _how the mean changes_ explains _all the changes in the distribution_.

However, this is often not the case: 
```{r}
incomeEx <- readr::read_csv(
    pathmaker(
        "data/incomeSurvey.csv"
      )
    )
incomeEx$income <- incomeEx$cinc/1000
ggplot(
  incomeEx
  ) +
  geom_histogram(
    aes(
      income
      ),
    # color = "grey50", 
    fill  = "grey70"
    ) + 
  geom_hline(
      yintercept = seq(
        0,
        2000,
        500
      ),
      color = "white"
  ) +
  labs(
    x = "Income",
    title = "Income (100s USD)"
    )
```

:::{callout.important}
Many variables are _not_ symmetric. Moreover, we are often not only interested in the _mean_, but in understanding _extremes_. Since linear regression model cannot give the full description of the distribution and everything is described by its mean, we need an alternative Framework.
:::

We do not need to forget that linear regression is appropriate for several applications and comes with many practical and theoretical reason to explain its widespread adoption. For example, it is computationally feasible and easy to implement and can be adapted to handle heteroskedasticity and robustness. Moreover, it can be _generalized_ to handle the non-gaussian case and even non-linearity^[Generalized Linear Models.].

# Quantiles 

_Quantiles_ are defined mathematically as:

$$
Q(p) = \mathrm{inf} \{ x: P(X \leq x) \geq p \}
$${#eq-quantile}

This is strictly related to the _cumulative distribution function_. The quantile of order $\alpha$ can also be interpred as that point in the distribution for which you have a probability $\alpha$ of observing a value less than that quantile^[In other words, it determines a specific point in the distribution. For example, the _median_ is the quantile of order 0.5: you have a $50%$ probability of observing a value lower than the median.].

We can compute a set of quantiles to help prove the point. Note that the mean of the distribution is `r mean(incomeEx$income)`.

```{r}
pquant <- quantile(incomeEx %>% pull(income), c(0.1,0.25,0.5,0.75,0.9))
pquant
```

There is a noticeable positive difference between _median_ and _mean_, indicating a right-skewed distribution. Computing such differences helps in understanding the _shape_ of our distribution.

We notice that the difference between the 75th and the 50th quantile is larger than the difference between the 50th and the 25th percentile: this is an indication of an asymmetry in the distribution. The same goes when comparing the the more extreme tails (e.g. the 10th and 90th quantile): 

```{r}
pquant[4]- pquant[3]; pquant[3] - pquant[2]  
pquant[5]- pquant[3]; pquant[3] - pquant[1]
```

Quantiles are not only _theoretical_: we can build an _Empirical Cumulative Distribution Function_ that describes the cumulative _frequencies_ of the observed values in our data.

For example, for an $iid$ sample:

$$
\hat F = \frac{\mathrm{number \ of \ observations} \leq t}{n}
$${#eq-ECDF}

## Sample Quantiles and Sampling Distribution

:::{.callout-warning}
If we wish to make inference we need to account for uncertainty.
:::

The quantiles can be _estimated_ by inverting the _ECDF_: this amounts to finding $Q(p)$ for a given $p$ and, even if the function is _discrete_, we can extrapolate between the jumps. However, we need to take into account the sampling distribution to make any inference about the characteristic of interest and their distribution shape in the population we are sampling from.

Asymptotic results shows that, for an _identically and independently distributed_ ($iid$) sample, being $f(x)$ the _probability density function_ and $Q(p)$ the _quantile function_:

$$
\hat Q \stackrel{\mathrm{approx}}{\sim} N \left ( Q(p), \sqrt{\frac{(1- p)p}{n} \frac{1}{f(Q(p))^2}} \right )
$${#eq-quantileasymptotic}

This function depends on the unknown: probability density function, $n$, and $p$.

## Quantiles as a Minimization Problem

Quantiles can be considered as a solution to a certain minimization problem; for example, the mean is defined as:

$$
x_{0.5} = \underset{c}{\mathrm{argmin}} \ \mathbb{E} [ \ \vert Y - c \vert \ ]
$${#eq-medianargmin}

A more general formula is:

$$
Q(p) = \underset{c}{\mathrm{argmin}} \ \mathbb{E} [d_p(Y, c)]
$${#eq-quantileargmin1}

Where:

$$
d_p (Y, c) = 
\begin{cases}
\begin{aligned}
(1 -p) \ \vert Y - c \vert \qquad \mathrm{for} \ Y &< c \\
p \ \vert Y - c \vert \qquad \mathrm{for} \ Y &\geq c
\end{aligned}
\end{cases}
$${#eq-quantileargmin2}

If we are dealing with _sample_ quantiles instead, we find $Q(p)$ as the value that minimises:

$$
\frac{1}{n} \sum^n_{i=1} d_p(y_i, c) = \frac{1 - p}{n} \sum_{y_i < q} \ \vert y_i - q \vert + \frac{p}{n} \sum_{y_i > q} \ \vert y_i - q \vert
$${#eq-quantilesample}

Each point contributes to a certain value and we can represent a _loss function_.

## Advantages of quantiles

- The mean, standard deviantion, and skewness are _not robust_: they are highly sensitive to _outliers_.
- Quantiles are _equivariant_ to non-decreasing transformation.

A function is non decreasing if $h(b) \geq h(a)$ if $b>a$^[The basic example is a _linear transformation_, but this property also applies to _logarithms_.]. This property implies that $Q(p, h(Y)) = h(Q(p, Y))$^[For example, $Q(p, a + bY) = a + bQ(p, Y)$.]. 

## Quantile-based Summary Statistics

Quantiles allow to describe the _scale_ and _shape_ of distributions.

Scale can be summarised with a _quantile-based scale_ measure (__QSC__) at a selected $p$:

$$
QSC(p) = Q(1 - p) - Q(p) \ \mathrm{for} \ p < 0.5
$${#eq-QSC}

The _interquartile range_ (IQR) corresponds to QSC(0.25).

Skewness can be summarised with a _quantile-based skewness_ (__QSK__) or _asymmetry_ index:

$$
QSK(p) = \frac{Q(1 - p) - Q(.5)}{Q(.5) - Q(p)} - 1\ \mathrm{for} \ p < 0.5
$${#eq-QSK}

If $QSK(p) > 0$, we have _right-skewness_.

# Quantile regression

> We are modeling the $\tau$ quantile of a distribution as a function of the _conditional quantile_ $X$.

The _linear quantile regression_ model (__QRM__) is:

$$
Q_{\tau} (Y \vert X) = \beta_0^{(\tau)} + \beta_1^{(\tau)} X
$${#eq-QRMcond}

If we assume that $\epsilon^{(\tau)}$ is a _random variable_ such that $Q_{\tau} (\epsilon^{(\tau)} \vert X) = 0$ we can rewrite the previous equation as:

$$
Y = \beta_0^{(\tau)} + \beta_1^{(\tau)} X + \epsilon^{(\tau)}
$${#eq-QRMlin}

Choosing $\tau$ determines the type of QRM: for $\tau = 0.5$, we have a _median regression_.

## Estimation

If we have pairs of independently sampled observations $(x_i, y_i)$, for $i = 1, ..., n$, the QRM leads to $y_i = \beta_0^{(\tau)} + \beta_1^{(\tau)} x_i + \epsilon_i^{(\tau)}$.

Based on the sampled points, we need a way to fit the function: we need an estimator for $\beta^{(\tau)} = (\beta_0^{(\tau)}, \beta_1^{(\tau)})$.

The median regression model estimates are such as to minimise^[This is the same _loss function_ used to find the median.s]:

$$
\sum^n_{i = 1} \vert y_i - a - bx_i \vert
$${#eq-QRMmedianloss}

More generally:

$$
\sum^n_{i = 1} d_{\tau} (y_i, a + b x_i)
$${#eq-QRMloss1}

where:

$$
d_{\tau} (y_i, a + b x_i) =
\begin{cases}
\begin{aligned}
(1 - \tau) \vert y_i - a - bx_i \vert \qquad \mathrm{for} \ y_i < a + b x_i \\
\tau \vert y_i - a - bx_i \vert \qquad \mathrm{for} \ y_i \geq a + b x_i
\end{aligned}
\end{cases}
$${#eq-QRMloss2}


:::{.callout-note}
Minimisation is achieved with _linear programming_ methods. The solution _might be not unique_.
:::

## Example

We will use Engel food expenditure data used in Koenker and Bassett(1982). This is a regression dataset consisting of 235 observations on income and expenditure on food for Belgian working class households.

```{r}
data(engel)
```

Plotting the dataset:

```{r}
ggplot(
  data = engel, 
  aes(
    x = income, 
    y = foodexp
    )
  ) + 
  geom_point() + 
  labs(
    title = "Engel food expenditure data"
    )
```

We need the `quantreg` dataset to perform QRM. The function `geom_quantile()` can be used to compare the standard linear regression fit and the median fit:

```{r}
ggplot(
  data = engel, 
  aes(
    x = income,
    y = foodexp
    )
  ) + 
  geom_point() + 
  geom_smooth(
    method = "lm", 
    aes(
      col = "limegreen"
      ), 
    se = FALSE
    ) + 
  geom_quantile(
    quantiles = 0.5, 
    aes(
      col = "dodgerblue"
      )
    ) +
  labs(
    title = "Engel food expenditure data - mean and median regression"
    ) +
  scale_colour_manual(
    name = 'Regression type:',
    values =c(
      'limegreen'='limegreen',
      'dodgerblue'='dodgerblue'
      ), 
    labels = c(
      'Linear Regression','
      Median Regression'
      )
    )
```

The following plot represents a set of _deciles regressions_:
  
```{r}
ggplot(
  data = engel, 
  aes(
    x = income,
    y = foodexp
    )
  ) + 
  geom_point() + 
  labs(
    title = "Engel food expenditure data"
    ) +
  geom_quantile(
    quantiles = seq(
      .1,
      .9,
      by=0.1
      ), 
    col = "indianred"
    )
```
  
> With this approach we can clearly see that expenditure on food is _not_ the same in the different quantiles.