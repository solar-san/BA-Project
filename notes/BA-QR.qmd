---
title: "Quantile Regression"
author: "solar-san"
date-modified: "`r Sys.Date()`"
format:
  html:
    theme: github
    toc: true
    toc-location: right
    margin-header: "BA-Project_header.png"
    fig-align: center
    fig-width: 10
    fig-height: 8
    df-print: paged
    html-math-method: katex
    code-overflow: scroll
    code-copy: hover
    code-fold: show
    highlight-style: breeze
    tbl-cap-location: top
    fig-cap-location: top
    citations-hover: true
    footnotes-hover: true
    header-includes: |
      <meta name="author" content="solar-san">
      <meta name="image" property="https://raw.githubusercontent.com/solar-san/BA-Project/main/docs/figures/BA-Project_header.png">
      <meta 
      name="description"
      content="Notes on quantile regression for business analysts. It contains a self-contained introduction tje topic, with R code snippets and the necessary mathematical and statistical notation.">
      <link rel="preconnect" href="https://fonts.googleapis.com">
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&family=Fira+Code&display=swap" rel="stylesheet">
mainfont: "Atkinson Hyperlegible"
monofont: 'Fira Code'
---

![](https://raw.githubusercontent.com/solar-san/BA-Project/main/docs/figures/BA-Project_header.png)

```{r setup}
#| echo: false
#| results: hide
#| warning: false
knitr::opts_chunk$set(
  echo = T
)

  
pathmaker <- function(filename, data_path = "/Users/themagician/Documents/DABS/Appunti/Business_Analytics/Lab3/") {
  file_path <- paste0(
    data_path,
    filename
  )
  return(
    file_path
  )
}


lib_list <- list(
  "tidyverse",
  "quantreg",
  "broom",
  "ggthemes",
  "patchwork"
)

lapply(
  lib_list,
  require,
  quietly = TRUE, 
  warn.conflicts = FALSE, 
  character.only = TRUE
)

theme_set(
  theme_tufte(
    base_size = 20,
    base_family = "Atkinson Hyperlegible"
  )
)

theme_update(
    legend.position = "top"
)
```

# Introduction {#sec-intro}

> What is _Business Analytics_? It is an applied discipline, tasked with gaining insight into business operations and helping to make better, fact-based decisions: a basic pillar of being _data-driven_. It combines Statistics, Computer Sciences, Management Science, Operational Research, and much more.

It has three main components:

1. __Descriptive analytics__: providing insight into business questions by summarizing and visualizing observed data. Typically based on simple summaries and charts^[Any predictive or prescriptive model is based on some steps of _descriptive analytics_.].
2. __Predictive analytics__: providing insight into the possible future by extrapolating patterns found in the historical data. Typically making use of statistical or machine learning predictive models (remember the Statistical Learning and Data Analytic courses).
3. __Prescriptive analytics__: providing ways to make the best possible decisions based on the available information.

> What is the role of Quantile Regression in Business Analytics?

The Ordinary-Least-Squares (OLS) based linear regression is a well-known approach when it comes to model relationships between variables, such as:

$$
Y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{p i + \epsilon_i}
$${#eq-multlinreg}

There are fundamental assumptions behind this model:

- Independence of observations and error terms.
- The error terms are identically distributed.
- We have _homoskedasticity_ and _zero mean_^[This last one, however, is not strictly necessary.].
- Normality of errors, as in $\epsilon \stackrel{iid} {\sim} N(0, \sigma^2_{\epsilon})$^[This assumption allows for inference on the model parameters.].

The main focus is to estimate the _mean_ of the distribution: all the assumptions lead to the following formula.

$$
\mathbb{E} [Y_i \vert X = x_i] = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p
$${#eq-condlinreg}

> Quantile regression allows to deal with data that cannot be described with a _normal distribution_ and that can be summarised effectively using the _mean_^[Warning: the mean is _not_ a robust indicator of position.].

The main consequence of having homoskedastic and normal errors are:

- Symmetry.
- If the mean and scaled are known, we can assume that we know the whole distribution. Therefore, knowing _how the mean changes_ explains _all the changes in the distribution_.

However, this is often not the case. 

> This dataset contains information from the 2001 panel of the _Survey of Income and Program Participation_ (SIPP). The dataset contains information on several American households and on the household annual income in 2001.

```{r}
incomeEx <- readr::read_csv(
    pathmaker(
        "data/incomeSurvey.csv"
      )
    )
incomeEx$income <- incomeEx$cinc/1000
ggplot(
  incomeEx
  ) +
  geom_histogram(
    aes(
      income
      ),
    # color = "grey50", 
    fill  = "grey70"
    ) + 
  geom_hline(
      yintercept = seq(
        0,
        2000,
        500
      ),
      color = "white"
  ) +
  labs(
    x = "Income",
    title = "Income (100s USD)"
    )
```

:::{callout.important}
Many variables are _not_ symmetric. Moreover, we are often not only interested in the _mean_, but in understanding _extremes_. Since linear regression model cannot give the full description of the distribution and everything is described by its mean, we need an alternative Framework.
:::

We do not need to forget that linear regression is appropriate for several applications and comes with many practical and theoretical reason to explain its widespread adoption. For example, it is computationally feasible and easy to implement and can be adapted to handle heteroskedasticity and robustness. Moreover, it can be _generalized_ to handle the non-gaussian case and even non-linearity^[Generalized Linear Models.].

# Quantiles 

_Quantiles_ are defined mathematically as:

$$
Q(p) = \mathrm{inf} \{ x: P(X \leq x) \geq p \}
$${#eq-quantile}

This is strictly related to the _cumulative distribution function_. The quantile of order $\alpha$ can also be interpred as that point in the distribution for which you have a probability $\alpha$ of observing a value less than that quantile^[In other words, it determines a specific point in the distribution. For example, the _median_ is the quantile of order 0.5: you have a $50%$ probability of observing a value lower than the median.].

We can compute a set of quantiles to help prove the point. Note that the mean of the distribution is `r mean(incomeEx$income)`.

```{r}
pquant <- quantile(incomeEx %>% pull(income), c(0.1,0.25,0.5,0.75,0.9))
pquant
```

There is a noticeable positive difference between _median_ and _mean_^[The mean of this distribution is `r mean(incomeEx %>% pull(income))`.], indicating a right-skewed distribution. Computing such differences helps in understanding the _shape_ of our distribution.

Moreover, the difference between the 75th and the 50th quantile is larger than the difference between the 50th and the 25th percentile: this is an indication of an _asymmetry_ in the distribution. The same goes when comparing the the more extreme tails (e.g. the 10th and 90th quantile): 

```{r}
#Difference between the 75th and 50th and the 50th and 25th quartiles:
pquant[4]- pquant[3]; pquant[3] - pquant[2]  
#Difference between the 90th and 50th and the 50th and 10th quartiles:
pquant[5]- pquant[3]; pquant[3] - pquant[1]
```

:::{.callout-tip}
Comparing the _mean_ and the _median_ helps in spotting _skewness_: a __right-skewed__ distribution will have a _positive_ difference between them, while a _negative_ difference signals a __left-skewed__ distribution. Comparing the difference between quartiles, or between deciles and quartiles, instead, indicates _asymmetry_: in an __asymmetrical__ distribution, these differences will be uneven and wide.
:::

Quantiles are not only _theoretical_: we can build an _Empirical Cumulative Distribution Function_ that describes the cumulative _frequencies_ of the observed values in our data.

For example, for an $iid$ sample:

$$
\hat F = \frac{\mathrm{number \ of \ observations} \leq t}{n}
$${#eq-ECDF}

## Sample Quantiles and Sampling Distribution

:::{.callout-warning}
If we wish to make inference we need to account for uncertainty.
:::

The quantiles can be _estimated_ by inverting the _ECDF_: this amounts to finding $Q(p)$ for a given $p$ and, even if the function is _discrete_, we can extrapolate between the jumps. However, we need to take into account the sampling distribution to make any inference about the characteristic of interest and their distribution shape in the population we are sampling from.

Asymptotic results shows that, for an _identically and independently distributed_ ($iid$) sample, being $f(x)$ the _probability density function_ and $Q(p)$ the _quantile function_:

$$
\hat Q \stackrel{\mathrm{approx}}{\sim} N \left ( Q(p), \sqrt{\frac{(1- p)p}{n} \frac{1}{f(Q(p))^2}} \right )
$${#eq-quantileasymptotic}

This function depends on the unknown: probability density function, $n$, and $p$.

## Quantiles as a Minimization Problem

Quantiles can be considered as a solution to a certain minimization problem; for example, the mean is defined as:

$$
x_{0.5} = \underset{c}{\mathrm{argmin}} \ \mathbb{E} [ \ \vert Y - c \vert \ ]
$${#eq-medianargmin}

A more general formula is:

$$
Q(p) = \underset{c}{\mathrm{argmin}} \ \mathbb{E} [d_p(Y, c)]
$${#eq-quantileargmin1}

Where:

$$
d_p (Y, c) = 
\begin{cases}
\begin{aligned}
(1 -p) \ \vert Y - c \vert \qquad \mathrm{for} \ Y &< c \\
p \ \vert Y - c \vert \qquad \mathrm{for} \ Y &\geq c
\end{aligned}
\end{cases}
$${#eq-quantileargmin2}

If we are dealing with _sample_ quantiles instead, we find $Q(p)$ as the value that minimises:

$$
\frac{1}{n} \sum^n_{i=1} d_p(y_i, c) = \frac{1 - p}{n} \sum_{y_i < q} \ \vert y_i - q \vert + \frac{p}{n} \sum_{y_i > q} \ \vert y_i - q \vert
$${#eq-quantilesample}

Each point contributes to a certain value and we can represent a _loss function_.

## Advantages of quantiles

- The mean, standard deviantion, and skewness are _not robust_: they are highly sensitive to _outliers_.
- Quantiles are _equivariant_ to non-decreasing transformation.

A function is non decreasing if $h(b) \geq h(a)$ if $b>a$^[The basic example is a _linear transformation_, but this property also applies to _logarithms_.]. This property implies that $Q(p, h(Y)) = h(Q(p, Y))$^[For example, $Q(p, a + bY) = a + bQ(p, Y)$.]. 

## Quantile-based Summary Statistics {#sec-quantilebasedssunivariate}

Quantiles allow to describe the _scale_ and _shape_ of distributions.

Scale can be summarised with a _quantile-based scale_ measure (__QSC__) at a selected $p$:

$$
QSC(p) = Q(1 - p) - Q(p) \ \mathrm{for} \ p < 0.5
$${#eq-QSC}

The _interquartile range_ (IQR) corresponds to QSC(0.25).

Skewness can be summarised with a _quantile-based skewness_ (__QSK__) or _asymmetry_ index:

$$
QSK(p) = \frac{Q(1 - p) - Q(.5)}{Q(.5) - Q(p)} - 1\ \mathrm{for} \ p < 0.5
$${#eq-QSK}

If $QSK(p) > 0$, we have _right-skewness_.

### Computing the QSK and QSC

Back at the SIPP example^[@sec-intro.], computing the QSC(.25) and the QSK(.1) will help to determine the scale and shape of the `income` distribution.

- QSC(.25):

```{r}
(pquant[4]- pquant[3])/(pquant[3] - pquant[2]) - 1
```

- QSK(.1):

```{r}
(pquant[5]- pquant[3])/(pquant[3] - pquant[1]) - 1
```

Both these values are _positive_: this indicates a _right-skewed_ distribution.

### Analysing Known Distributions

In this second example, we will compare two known distributions that have the _same mean_ but different characteristics, the Gamma and the Normal, sampling values from them.

```{r}
df <- data.frame(
  x = seq(
    -100,
    450
    )
  )
ggplot(df) + 
  geom_line(
    aes(
      x = x, 
      y = dnorm(
        x, 
        47.5, 
        34.2
        ),
      ),
      col = "limegreen"
    ) + 
  geom_line(
    aes(
      x = x, 
      y = dgamma(
        x, 
        1.9, 
        rate = 0.04
        ),
      ),
      col = "#004D73"
    ) +
  labs(
    title = "Gamma and Gaussian Densities",
    y = expression(f[x](x))
  )
```

The following code computes the relevant quartiles and deciles:
```{r}
pn <- qnorm(c(0.1,0.25,0.5,0.75,0.9), 47, 34.2)
pg <- qgamma(c(0.1,0.25,0.5,0.75,0.9), 1.9, rate = 0.04)
```

We can immediately confirm that the Normal density is perfectly symmetric^[Well, no surprises here.].

```{r}
pn[4] - pn[3]; pn[3] - pn[2]
```

```{r}
pn[5] - pn[3]; pn[3] - pn[1]
```

The Gamma density is a whole other story:

```{r}
pg[4] - pg[3]; pg[3] - pg[2]
```

```{r}
pg[5] - pg[3]; pg[3] - pg[1]
```

Comparing the QSK(0.1) value for the two distributions confirms our result:

```{r}
(pn[5]-pn[3])/(pn[3]-pn[1]) - 1
```

```{r}
(pg[5]-pg[3])/(pg[3]-pg[1]) - 1 
```

# Quantile regression

> We are modeling the $\tau$ quantile of a distribution as a function of the _conditional quantile_ $X$.

The _linear quantile regression_ model (__QRM__) is:

$$
Q_{\tau} (Y \vert X) = \beta_0^{(\tau)} + \beta_1^{(\tau)} X
$${#eq-QRMcond}

If we assume that $\epsilon^{(\tau)}$ is a _random variable_ such that $Q_{\tau} (\epsilon^{(\tau)} \vert X) = 0$ we can rewrite the previous equation as:

$$
Y = \beta_0^{(\tau)} + \beta_1^{(\tau)} X + \epsilon^{(\tau)}
$${#eq-QRMlin}

Choosing $\tau$ determines the type of QRM: for $\tau = 0.5$, we have a _median regression_.

## Estimation

If we have pairs of independently sampled observations $(x_i, y_i)$, for $i = 1, ..., n$, the QRM leads to $y_i = \beta_0^{(\tau)} + \beta_1^{(\tau)} x_i + \epsilon_i^{(\tau)}$.

Based on the sampled points, we need a way to fit the function: we need an estimator for $\beta^{(\tau)} = (\beta_0^{(\tau)}, \beta_1^{(\tau)})$.

The median regression model estimates are such as to minimise^[This is the same _loss function_ used to find the median.s]:

$$
\sum^n_{i = 1} \vert y_i - a - bx_i \vert
$${#eq-QRMmedianloss}

More generally:

$$
\sum^n_{i = 1} d_{\tau} (y_i, a + b x_i)
$${#eq-QRMloss1}

where:

$$
d_{\tau} (y_i, a + b x_i) =
\begin{cases}
\begin{aligned}
(1 - \tau) \vert y_i - a - bx_i \vert \qquad \mathrm{for} \ y_i < a + b x_i \\
\tau \vert y_i - a - bx_i \vert \qquad \mathrm{for} \ y_i \geq a + b x_i
\end{aligned}
\end{cases}
$${#eq-QRMloss2}


:::{.callout-note}
Minimisation is achieved with _linear programming_ methods. The solution _might be not unique_.
:::

## Example: Engel Food Expenditure Data

We will use Engel food expenditure data used in Koenker and Bassett(1982). This is a regression dataset consisting of 235 observations on income and expenditure on food for Belgian working class households.

```{r}
data(engel)
```

Plotting the dataset:

```{r}
ggplot(
  data = engel, 
  aes(
    x = income, 
    y = foodexp
    )
  ) + 
  geom_point() + 
  labs(
    title = "Engel food expenditure data"
    )
```

We need the `quantreg` dataset to perform the quantile regression analysis. The function `geom_quantile()` can be used to compare the standard linear regression fit and the median fit:

```{r}
ggplot(
  data = engel, 
  aes(
    x = income,
    y = foodexp
    )
  ) + 
  geom_point() + 
  geom_smooth(
    method = "lm", 
    aes(
      col = "limegreen"
      ), 
    se = FALSE
    ) + 
  geom_quantile(
    quantiles = 0.5, 
    aes(
      col = "dodgerblue"
      )
    ) +
  labs(
    title = "Engel food expenditure data - mean and median regression"
    ) +
  scale_colour_manual(
    name = 'Regression type:',
    values =c(
      'limegreen'='limegreen',
      'dodgerblue'='dodgerblue'
      ), 
    labels = c(
      'Linear Regression',
      'Median Regression'
      )
    )
```

The following plot represents a set of _deciles regressions_:
  
```{r}
ggplot(
  data = engel, 
  aes(
    x = income,
    y = foodexp
    )
  ) + 
  geom_point() + 
  labs(
    title = "Engel food expenditure data"
    ) +
  geom_quantile(
    quantiles = seq(
      .1,
      .9,
      by=0.1
      ), 
    col = "indianred"
    )
```
  
> With this approach we can clearly see that expenditure on food is _not_ the same in the different quantiles.

By default `geom_quantile` shows the quartile fits^[$\tau = c(0.25, 0.5, 0.75)$.].

```{r}
ggplot(
    data = engel, 
    aes(
        x = income, 
        y = foodexp
        )
    ) + 
    geom_point() + 
    labs(
        title = "Engel food expenditure data"
        ) +
    geom_quantile(
        col = "dodgerblue"
        )
```

> What are the implications of different $\beta_1^{(\tau)}$ slopes?

If we take two models for $\tau = p$ and $\tau = q$ and rearrange the two equations, having the same data points $(y_i, x_i)$, we obtain:

$$
(\epsilon_i^{(p)} - \epsilon_i^{(q)}) = (\beta_0^{(p)} - \beta_0^{(q)}) + (\beta_1^{(p)} - \beta_1^{(q)})x_i
$${#eq-slopedifferences1}

The difference between the two errors depend on $x_i$. If the two slopes are equal^[$(\beta_1^{(p)} - \beta_1^{(q)}) = 0$.], then the two errors differ only by a constant: hence, the quantile error distribution only differ for a constant. This means that the observations are identically distributed for different values of $X$.

:::{.callout-important}
QRM can be used to assess whether the assumption of _identically distributed_ observations is valid.
:::

Note that, however, to formally test the equality of slopes we need a way to make _inference_^[@sec-inference.].

We can estimate the QR with the `quantreg` package, calling the `rq` function^[It works similarly to the well-known `lm` function.].

```{r}
rqfit_engel <- rq(
  foodexp ~ income, 
  data = engel
  )
```

```{r}
rqfit_engel %>% 
  summary()
```

We will also use the `broom` library to tidy the model output.

```{r}
rqfit_engel %>% tidy()
```

`tidy` summarizes information about the components of a model.

```{r}
rqfit_engel %>% glance()
```

`glance` accepts a model object and returns a `tibble::tibble()` with exactly one row of model summaries. The summaries are typically goodness of fit measures, p-values for hypothesis tests on residuals, or model convergence information.

```{r}
rqfit_engel %>% augment()
```

Augment accepts a model object and a dataset and adds information about each observation in the dataset. Most commonly, this includes predicted values in the `.fitted` column, residuals in the `.resid` column, and standard errors for the fitted values in a `.se.fit` column. New columns always begin with a `.` prefix to avoid overwriting columns in the original dataset.

We could also want to estimate a quantile regression model for several quantiles: this can be done with a unique call using the option `tau`:

```{r}
rqmultfit_engel <- rq(
  foodexp ~ income, 
  data = engel, 
  tau = c(
    0.1, 
    0.25, 
    0.5, 
    0.75, 
    0.9
    )
  )
coef(rqmultfit_engel)
```

```{r}
summary(rqmultfit_engel)
```

To plot these different models:

```{r}
aug1 <- rqmultfit_engel %>% 
  augment()
ggplot(aug1) + 
  geom_point(
    aes(
      x = income,
      y = foodexp
    ),
    col = "gray40",
    data = engel
  ) +
  geom_line(
    aes(
      x = income, 
      y =.fitted, 
      col = .tau
      )
    ) +
  scale_color_brewer(
    type = "div",
    palette = "Dark2",
    aesthetics = "colour"
  )
```

We could also plot the slope estimate as a function of $\tau$:

```{r}
#| fig-width: 10
#| fig-height: 6
rqmultfit_engel %>% 
  tidy() %>% 
  filter(
    term == "income"
    ) %>% 
  select(
    tau, 
    estimate
    ) %>% 
  ggplot(
    aes(
      x = tau, 
      y = estimate
      )
    ) +
  geom_point() + 
  geom_line() +
  labs(
    title = "Slope Estimate as a Function of the Quantile"
    )
```

Note that the usual `R` methods are available, such as `predict` (to derive predicted values) and `residuals` (to compute the residuals).

```{r}
rqmultfit_engel %>% 
  predict() %>% 
  head()
```

```{r}
rqmultfit_engel %>% 
  residuals() %>% 
  head()
```
```{r}
rqmultfit_engel %>% 
  augment() %>% 
  ggplot() + 
  geom_point(
    aes(
      x = income, 
      y = .resid
      )
    ) +
  geom_hline(
    yintercept = 0,
    color = "grey80",
    linetype = "dashed"
  ) +
  facet_wrap(
    ~.tau,
    nrow = 3,
    ncol = 2
    ) +
  labs(
    title = "Residual Plot for Different Quantiles"
  )
```


## Multiple Independent variables

We can extend the model and obtain a _multivariate_ quantle regression. In the (compact) matrix notation we have:

$$
Q_{\tau} = (Y \vert \mathbf{X}) = \mathbf{X} \mathbf{\beta}^{(\tau)} + \epsilon_i^{\tau}
$${#eq-multiQRM}

:::{.callout-tip}
The coefficients $\mathbf{\beta}^{(\tau)}$ can be interpreted as the effect of $x$ on the quantile once the effect of other variables is also taken into account.
:::

Again, we need a way to assess the significance of each added variable: in other words, a way to make _inference_. 

## Inference {#sec-inference}

### $iid$ case

> Does the addition of a variable $x_j$ _improve_ the goodness of fit?

There are two approaches to conduct inference in the context of quantile regression:

1. With lots of mathematics we can derive some asymptotic results for $\beta^{(\tau)}$.
2. Another option is to use _simulation based inference_, such as __the bootstrap__.

For $iid$ observations it can be shown that, under some weak conditions:

$$
\sqrt{n} (\hat \beta - \beta) \stackrel{d} {\to} N( 0, \mathbf{\Sigma}) 
$${#eq-inference}

$\mathbf{\Sigma}$ is the _variance-covariance_ matrix of form:

$$
\mathbf{\Sigma} = \frac{\tau(1 - \tau)}{f_{\epsilon (\tau)}} (\mathbf{X}^T \mathbf{X})^{-1}
$${#eq-inferencevarcovarmatrix}

The term $f_{\epsilon (\tau)}$ is the _pdf_ of the error term $\epsilon (\tau)$ evaluated at the $\tau^{th}$ quantile of the error distribution, which we assumed to be 0.

### Non-$iid$ case

The form of $\mathbf{\Sigma}$ for the non-identically distributed case is fairly complex.

In general, the mathematics is difficult and it is not always possible to derive asymptotic variances analytically. Hence, a simulation-based method such as the __bootstrap__ is required.

## The Bootstrap

> __Bootstrapping__^[First introduced by Bradley Efron in the late 70s.] is _any test or metric that uses random sampling with replacement_, mimicking the sampling process, and falls under the broader class of _resampling methods_. Bootstrapping assigns _measures of accuracy_^[bias, variance, confidence intervals, prediction error, etc.] to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.

It is based on the sample data and having _enough_ computing power. The key idea is that we assume the population is just many copies of the sample; then, we make lots of copies of the sample and then we sample repeatedly from the population. In practice, instead of actually making many copies of the sample and sampling from that, we use a sampling technique that is equivalent, based on _sampling with replacement_.

In particular, we will use __non parametric resampling bootstrap__.

- We sample _with replacement_ from the original sample.
- Each sample selected is called a _bootstrap sample_.
- For each bootstrap sample, we compute the estimate of the parameter, giving us a _bootstrap estimate_.
- We collect the bootstrap estimates for many bootstrap samples to create a _bootstrap distribution_ which we use as a _sampling distribution_ for the estimate.

### Bootstrap Confidence Intervals

We can construct confidence intervals by bootstrap. 

> When a bootstrap distribution for an estimate is __symmetric__ and __bell-shaped__, we calculate a $1 − \alpha$% confidence interval using:

$$
\left [\hat \theta - q_{1- \frac{\alpha}{2}} \times \mathrm{sd_{boot}}, 
\hat \theta + q_{1- \frac{\alpha}{2}} \times \mathrm{sd_{boot}} \right ]
$${#eq-CIsymmetricbootstrap}

where $\mathrm{sd_{boot}}$ denotes the standard deviation of the bootstrap estimates.

> When a bootstrap distribution for an estimate is ___not_ symmetric__ we calculate a $1 − \alpha$% confidence interval using:

$$
\left [\theta_{\mathrm{boot}} \left (\frac{\alpha}{2} \right),
\theta_{\mathrm{boot}}\left (1 - \frac{\alpha}{2} \right) \right]
$${#eq-CIasymmetricbootstrap}

These two extremes denote the $\alpha/2$ and $1 - \alpha/2$ _empirical quantiles_ of the bootstrap estimates.

### Bootstrap Inference

Using the _standard error_ derived with the bootstrap samples we can build a test on $\beta_j^{\tau}$^[In the following notation the $\tau$ is omitted for readability.], in the form 

$$
\begin{cases}
\begin{aligned}
H_0: B_j = 0 \\
H_1 : B_j \neq 0 
\end{aligned}
\end{cases}
$${#eq-bootstrapinference}

Under the null hypothesis, our null statistic is:

$$
TS_{B_j} = \frac{\hat \beta_j}{se(B_j)} \sim N(0, 1)
$${#eq-teststatistic}

:::{.callout-important}
We estimate $se(\beta_j)$ with the bootstrap or with some theoretical derivation: hence, we need to compare the test statistic against a $T$ distribution with $n − (p + 1)$ degrees of freedom.
:::

Inference on single coefficient is often useful; however, we wish to test other hypothesis and also a to test _set_ of hypothesis.

For a QR model for a single quantile $\tau$ we can have two different _nested model_ structures; for $q \geq p$:

$$
\begin{aligned}
\mathrm{Model \ 0}: \qquad Q_{\tau} (Y \vert X_1, …, X_q) = \beta_0^{\tau} + \beta_1^{\tau} X_1 + … + \beta_q^{\tau} \\
\mathrm{Model \ 1}: \qquad Q_{\tau} (Y \vert X_1, …, X_p) = \beta_0^{\tau} + \beta_1^{\tau} X_1 + … + \beta_p^{\tau} \\
\end{aligned}
$${#eq-nestedmodels}

Note that _Model 1_ reduces to _Model 2_ when $\beta_{p + 1} = … = \beta_q = 0$. 

We are interested in constructing a test to verify the following null hypothesis^[This test amounts to asking the question: "Are the added variables _significant_ in explaining the behaviour of our dependent variable?"]:

$$
\begin{cases}
\begin{aligned}
H_0: B_{p + 1} = … = \beta_q = 0 \\
H_1: B_{p + 1} = … = \beta_q \neq 0
\end{aligned}
\end{cases}
$${#eq-nestedinference}

This null hypothesis can be tested through an ANOVA test, based on the asymptotic _normality_.

Another important test might involve the comparison of the slopes of $\beta^{(p)}_1$ and $\beta^{(q)}_1$, under the assumption of _identically distributed_ data. In this case, we are comparing models for _different quantiles_; if we have one explanatory variable, then:

$$
\begin{aligned}
\mathrm{Model \ 1}: Q_{q} (Y \vert X) = \beta_0^{(q)} + \beta_1^{(q)} X \\
\mathrm{Model \ 2}: Q_{p} (Y \vert X) = \beta_0^{(p)} + \beta_1^{(p)} X
\end{aligned}
$${#eq-slopecomparison}

We are interested in the following hypothesis:

$$
\begin{cases}
\begin{aligned}
H_0: B^{(p)}_1 = B^{(q)}_1 \\
H_1: B^{(p)}_1 \neq B^{(q)}_1
\end{aligned}
\end{cases}
$${#eq-slopetest}

Again, this test can be performed via ANOVA, based on the asymptotic normality.

:::{.callout-tip}
we should test for _more_ that two quantiles; formal testing gives _evidence_ of varying slopes.
:::

# More Intepretation of the Quantile Regression

> What are the implications of the alernative hypothesis $H_1: B^{(p)}_1 \neq B^{(q)}_1$?

The main implication of this null hypothesis is that the _distribution_ changes as a function of $X$. We can describe the change in _scale_ and _shape_ using _quantile-based_ measures; we already introduced them for the simple _univariate case_^[@sec-quantilebasedssunivariate.], and now we will extend them and compute them as a function of $\mathbf{X}$.

$$
\begin{aligned}
QSC(p \vert X)
&= Q(1 - p \vert X) - Q(p \vert X) = \\ 
&= (\beta{(1 - p)}_0 - \beta^{(1 - p)}_1 X) - (\beta{(p)}_0 - \beta^{(p)}_1 X) = \\
&=  (\beta{(1 - p)}_0 - \beta^{(p)}_0) + (\beta^{(1 - p)}_1 - \beta^{(p)}_1) X
\end{aligned}
$${#eq-QSCext}

:::{.callout-important}
If the scale is the same $\beta^{(1 - p)}_1 = \beta^{(p)}_1$; hence, $QSC(p \vert X)$ reduces to a constant.
:::

$$
\begin{aligned}
QSK(p \vert X)
&= \frac{Q(1-p \vert X) - Q(.5 \vert X)}{Q(.5 \vert X) - Q(p \vert X)} - 1 =\\
&= \frac{\beta^{(1 - p)}_0 + \beta^{(1 - p)}_1 X - \beta^{(.5)}_0 - \beta^{(.5)}_1 X}{\beta^{(.5)}_0 - \beta^{(.5)}_1 X -\beta^{(p)}_0 + \beta^{(p)}_1 X  } -1 = \\
&= \frac{ (\beta^{(1 - p)}_0 - \beta^{(.5)}_0) + (\beta^{(1 - p)}_1 - \beta^{(.5)}_1) X}{ (\beta^{(.5)}_0 - \beta^{(p)}_0) + (\beta^{(.5)}_1 - \beta^{(p)}_1) X} -1
\end{aligned}
$${#eq-QSKext}

:::{.callout-important}
Under _symmetry_, $(\beta^{(1 - p)}_1 - \beta^{(.5)}_1) = (\beta^{(.5)}_1 - \beta^{(p)}_1)$ and $(\beta^{(1 - p)}_0 - \beta^{(.5)}_0) = (\beta^{(.5)}_0 - \beta^{(p)}_0)$; this implies that $QSK(p \vert X)$ reduces to 0.
:::