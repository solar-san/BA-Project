---
title: "Time Series Forecasting"
author: "solar-san"
date-modified: "`r Sys.Date()`"
format:
  html:
    theme: github
    toc: true
    toc-location: right
    margin-header: "BA-Project_header.png"
    fig-align: center
    fig-width: 10
    fig-height: 8
    html-math-method: katex
    code-overflow: scroll
    code-copy: hover
    code-fold: show
    highlight-style: breeze
    tbl-cap-location: top
    fig-cap-location: top
    citations-hover: true
    footnotes-hover: true
    header-includes: |
      <meta name="author" content="solar-san">
      <meta name="image" property="https://raw.githubusercontent.com/solar-san/BA-Project/main/docs/figures/BA-Project_header.png">
      <meta 
      name="description"
      content="Notes on time series analysis for business analysts. It contains a self-contained introduction to smoothings and decomposition, with R code snippets and the necessary mathematical and statistical notation.">
      <link rel="preconnect" href="https://fonts.googleapis.com">
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&family=Fira+Code&display=swap" rel="stylesheet">
mainfont: "Atkinson Hyperlegible"
monofont: 'Fira Code'
---

![](https://raw.githubusercontent.com/solar-san/BA-Project/main/docs/figures/BA-Project_header.png)

```{r setup}
#| echo: false
#| results: hide
#| warning: false
knitr::opts_chunk$set(
  echo = T
)

lib_list <- list(
  "tidyverse",
  "fpp3",
  "ggthemes",
  "patchwork"
)

lapply(
  lib_list,
  require,
  quietly = TRUE, 
  warn.conflicts = FALSE, 
  character.only = TRUE
)

theme_set(
  theme_tufte(
    base_size = 20,
    base_family = "Atkinson Hyperlegible"
  )
)

theme_update(
    legend.position = "top"
)
```

# Introduction

> What is _Business Analytics_? It is an applied discipline, tasked with gaining insight into business operations and helping to make better, fact-based decisions: a basic pillar of being _data-driven_. It combines Statistics, Computer Sciences, Management Science, Operational Research, and much more.

It has three main components:

1. __Descriptive analytics__: providing insight into business questions by summarizing and visualizing observed data. Typically based on simple summaries and charts^[Any predictive or prescriptive model is based on some steps of _descriptive analytics_.].
2. __Predictive analytics__: providing insight into the possible future by extrapolating patterns found in the historical data. Typically making use of statistical or machine learning predictive models (remember the Statistical Learning and Data Analytic courses).
3. __Prescriptive analytics__: providing ways to make the best possible decisions based on the available information.

> What is the role of Time Series Analytics and Forecasting in Business Analytics?

Time series analysis is crucial in Business Intelligence (BI) and Business Analytics for predicting future trends, understanding past patterns, and making informed decisions. It enables organizations to extract valuable insights from temporal data, aiding in strategic planning, resource allocation, and risk management. 

Accurate time series forecasting empowers businesses to optimize inventory, enhance production planning, and anticipate market fluctuations. 

Incorporating machine learning, econometrics, and statistical methods in time series analysis enhances the precision of predictions, providing a competitive edge. In BI, timely and reliable forecasts contribute to effective decision-making, improving overall business performance and adaptability to dynamic market conditions.

> __Syllabus__:

- Time series data.
- Patterns: seasonality and trends.
- Autocorrelation.
- Transformations.
- Decomposition.
- Exponential Smoothing methods.

# Forecasting and Time Series

Predictions have been a human interest throughout history. The ability to forecast depends on:

- How much we understand the factors at play, whether our model of the phenomenon is good or not.
- How much data is available.
- How similar the future will be to the past.
- The volatility of the process of interest.
- Whether the forecasts can affect the thing we are trying to forecast.

:::{.callout-caution}
_Data-based forecasts_ should be based on _relevant_ data of the _highest possible quality_.
:::

However, data are not always available.

- Qualitative forecasting is used when no data is available.
- Quantitative forecasting is used when relevant numerical data is available and is representative of some aspect of the past.

> _Quantitative_ methods _must be appropriate for the problem and for the data used to solve it_. Specific methods are used for _time series_ data.

In time series, _time is a defining property of the data_. While it is a continuous variable, sampling usually occurs at regularly spaced intervals, giving regular time steps that represent the _period_ of the data.

```{r TS example}
gafa_stock %>% 
  filter(
    Symbol == "GOOG",
    year(Date) == 2018
  ) %>% 
  autoplot(
    Close
  ) +
  labs(
    title = "Google Stock Price at Close",
    x = "Month",
    y = "Value at Close"
  )
```

# Forecasting: the Statistical Perspective

The phenomenon that we want to forecast can be conceptualized as a _stochastic process_, which is a _succession of stochastic variables_ $\{ y_t \}$. If we consider its value at time $T$, given $\mathcal{I}$ the set of all available observations for $t = 1, 2, …, T -1$, we can express the _forecast distribution_ as a _joint distribution_, conditional on the available information: $y_t \vert \mathcal{I}$. This notation means: "_the random variable $y_t$, given all that we know in $\mathcal{I}$".

- The _point forecast_ will be the _mean_ or the _median_ of $y_t \vert \mathcal{I}$.
- The _forecast variance_ is $V(y_t \vert \mathcal{I})$. 
- A prediction interval or _interval forecast_ is a range of $y_t$ values with high probability.

When we are dealing with _time series_, we have:

- $y_{t \vert t-1} = y_{t} \vert \{ y_1, y_2, …, y_{t-1} \}$.

A $h-step$ forecast that takes into account all observations up to time $T$ is expressed as:

$$
\hat y_{T + h \vert T} = \mathbb{E} (y_{T + h}\vert y_1, …, y_T)
$$ {#eq-hfor}


Models are useful to forecast because they account for _uncertainty_.

# Patterns

We can filter three different components of the time series. These are _patterns_ that can be used to forecast more effectively if identified correctly.

1. __Trend__: long-term increase or decrease in the data.
2. __Seasonal__: a series is influenced by a _seasonal factor_ at a _fixed period_ (quarterly, monthly, weekly, daily, hourly).
3. __Cyclic__: data exibit rises and falls that _are not of fixed period_.

:::{.callout-important}
Cyclic patterns have a duration of at least 2 years. 
Seasonal patterns cannot have a periodicity longer than the year and are of _constant length_.
The magnitude of a cycle is often more variable than the magnitude of a seasonal pattern.
:::

The main implication of the differences between seasonal and cyclic patterns is that the timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data.

```{r TS example seasonality}
#| fig-cap: "We can observe a quarterly season pattern in the Australian beer production."
#| fig-height: 6
aus_production %>% 
  filter(
    year(Quarter) >= 2000
  ) %>% 
  autoplot(Beer) +
  labs(
    title = "Australian beer production"
  )
```

```{r TS example cyclic + seasonal}
us_employment %>%
     filter(
        Title == "Retail Trade", 
        year(Month) >= 1980) %>%
    autoplot(
    Employed
    ) +
    geom_rangeframe() %>%
    labs(
    title = "US employment from 1980"
  )
```

# Autocorrelation

While in standard _regression models_ we assume _independence_ among the observations, this is not true with time series models. We can visualize this by comparing $y_t$ against $y_{t-k}$ for different values of $k$. $k$ is the _lag_ vaue, while $y_{t-k}$ the _lagged value_: we define _autocorrelation_ the linear dependence of a variable with its lagged values^[In other words: we are concerned with the dependency of a phenomenon with its own past.].

```{r TS example lag plot}
#| fig-width: 12
#| fig-height: 12
aus_production %>% 
  gg_lag(
    Beer,
    geom = "point"
  ) +
  labs(
    title = "Lag plot for Australian beer production"
  ) +
  theme(
    axis.text.x = element_text(angle = 270),
  )
```

From the following plot, we can see that at each 4th lag^[$k = 4$, or $t-4$] there is clear evidence of a correlation between $y_t$ and $y_{t-k}$. This is a tell-tale sign of _seasonality_. 

However, not always we can spot it using visualization techniques:

```{r TS example lag and ciclycality}
#| fig-width: 12
#| fig-height: 12
us_employment %>% 
  filter(
    year(Month) >= 2010,
    Series_ID == "CEU0500000001"
  ) %>%
  gg_lag(
    Employed,
    geom = "point"
  ) +
  labs(
    title = "Lag plot for Australian beer production"
  ) +
  theme(
    axis.text.x = element_text(angle = 270),
  )
```

:::{.callout-caution}
When _multiple patterns_ overlap, we cannot use these lag charts to spot _seasonality_.
:::

# Autovariance and Autocorrelation

> _Autocovariance_ and _autocorrelation_ measure linear relations between lagged values of a time series $y$.

- $c_k$: sample autocovariane at lag $k$.
- $r_k$: sample autocorrelation at lag $k$^[$r_k$ is almost the same as the _sample correlation_ between $y_t$ and $y_{t−k}$.].

$$
c_k = \frac{1}{T} \sum_{t = k + 1}^T (y_t - \bar y)(y_{t - k} - \bar y)
$${#eq-ck}

$$
r_k = \frac{c_k}{c_0}
$${#eq-rk}

We can visualize the autocorrelations with a _correlogram_ plot^[The correlation for $k=0$ is 0, therefore it is often omitted.]:

```{r ACF TS example 1}
#| fig-height: 6
aus_production %>% 
filter(
    year(Quarter) >= 2000
  ) %>%
    ACF(
        Beer,
        lag_max = 8
    ) %>%
    autoplot()
```

```{r ACF TS example 2}
us_employment %>%
    filter(
        Title == "Retail Trade", 
        year(Month) >= 1980
        ) %>%
        ACF(
            Employed,
            lag_max = 32
        ) %>%
        autoplot()
```

- When data have a trend, the autocorrelations for small lags tend to be large and positive.
- When data are seasonal, the autocorrelation will be larger at the seasonal lags^[This is visible at multiples of the seasonal frequency.].
- When data are trended and seasonal, you see a combination of these effects.

> Time series that show no autocorrelation are called _white noise_.

This is a benchmark: if a series has a small $r_k$, we can assume that behaves like white noise. The sampling distribution for white noise data is (asymptotically) $N(0, 1/T)$, therefore we can build _confidence intervals_, usually at $\alpha=5%$.

```{r WN plot}
tsibble(
    sample = 1:150, 
    wn = rnorm(150), 
    index = sample
    ) %>%
    autoplot(wn) + 
    labs(
        title = "White noise", 
        y = ""
    )
```


```{r WN ACF plot}
tsibble(
    sample = 1:150, 
    wn = rnorm(150), 
    index = sample
    ) %>%
    ACF() %>%
    autoplot()
```

We can see that even if there is slight evidence of correlation, the confidence intervals help in choosing to label the series as white noise: it is not statistically significant.

# Transformations

> _Transformations_ have many uses: adjusting the series helps with decomposition and comparisons; also, stabilizing the variance simplifies subsequent analysis.

We have four main classes of transformations, dealing with economic data:

1. Calendar adjustments.
2. Population adjustments.
3. Inflation adjustments.
4. Mathematical transformations.

```{r transformation example}
#| fig-height: 12
p1 <- global_economy %>% 
  filter(
    Country == "Australia" |
    Country == "Italy"
    ) %>% 
  autoplot(GDP) +
  labs(
    title= "GDP",
    y = "$US"
    ) +
  theme(
    legend.position = "none"
  )
p2 <- global_economy %>% 
 filter(
    Country == "Australia" |
    Country == "Italy"
    ) %>% 
  autoplot(Population) +
  labs(
    title= "Population",
    y = "$US"
    ) +
  theme(
    legend.position = "none"
  )
p3 <- global_economy %>% 
  filter(
    Country == "Australia" |
    Country == "Italy"
    ) %>% 
  autoplot(GDP/Population) +
  labs(
    title= "GDP per capita", 
    y = "$US"
    )

(p1 + p2) / p3 + plot_annotation(
  title = "Transformation example: from GDP to GDP per capita"
)
```

Mathematical transformations are useful to stabilize the variability of the time series, which should be constant: when it varies with the level of the series, a `log` transformation can eliminate the issue^[For the transformed data.], Changes in a log value are relative changes on the original scale.

$$
Y_t = b_t X_t \to log(Y_t) = log(b_t) * log(X_t)
$${#eq-logtransform}

```{r TS log transformation example}
#| fig-height: 12
p1 <- aus_production %>% 
  autoplot(Gas)
p2 <- aus_production %>% 
  autoplot(
    log(
      Gas
      )
    ) +
  labs(
    y = expression(log(Gas))
  )

p1 / p2 + plot_annotation(
  title = "Log transformations"
)
```

:::{.callout-tip}
Choosing an interpretable transformation is a great advantage. Simple transformations go a long way.
:::

# Decomposition

Time series values can be decomposed as a function of their patterns:

$$
y_t = f(S_t, T_t, R_t)
$${#eq-decompose}

We can identify a _seasonal_ and a _trend_ component; there is also a _remainder_ that accounts for the unexplained variability.

- $y_t$ = data at period $t$.
- $T_t$ = trend-cyvle component at period $t$.
- $S_t$ = seasonal component at period $t$.
- $R_t$ = remainder at period $t$.

> Additive decomposition:

$$
y_t = S_t + T_t + R_t
$${#eq-additive}

The additive model is appropriate if the magnitude of seasonal fluctuations does not vary with level.

> Multiplicative decompositions

$$
y_t = S_t \times T_t \times R_t
$${#eq-multiplicative}

If seasonal changes are proportional to the level of the series, then a multiplicative model is appropriate^[Anytime, a $log$ transformation can turn a multiplicative relationship into an additive relationship.].

There are several approaches to decomposition:

1. Classical Decomposition
2. Seasonal Decomposition of Time Series by Loess (STL)
3. X11/SEATS^[Used mostly by government agencies.].

The focus is on _classical decomposition_; it can be performed for additive and multiplicative seasonality. To estimate the _trend_ at time $t$, _Moving Averages_ (MA) of order $m$ are computed on the time series values.

```{r classical decomposition example 1}
 us_employment |>
     filter(
       Title == "Retail Trade", 
       year(Month) >= 1980
       ) %>% 
     model(
       classical_decomposition(
         Employed,
         type = "additive"
         )
       ) %>% 
     components() %>% 
     autoplot() +
  labs(
    x = "Year",
    title = "Classical additive decomposition \n of total US retail employment"
    ) 
```

```{r classical decomposition example 2}
aus_production %>% 
  filter(
    year(Quarter) >= 2000
  ) %>% 
  model(
       classical_decomposition(
         Beer,
         type = "additive"
         )
       ) %>% 
     components() %>% 
     autoplot() +
  labs(
    x = "Year",
    title = "Classical additive decomposition \n AUS beer production"
    ) 
```

The trend component of classical decomposition is estimated with a _moving average_.

# Moving Averages

A _moving average_ of order $m$ can be written as:

$$
\hat T_t = \frac{1}{M} \sum_{j = -k}^k y_{t + j} \qquad m = 2k + 1
$${#eq-MA}

$m$ is _odd_, therefore the MA is _symmetric_ and centered at time $t$; it is a paramenter that controls _how smooth the estimate is_^[For $m=1$ we do not have smoothing; for $m=n/2$ we are smoothing every information out but the mean.].

```{r MA example 1}
global_economy %>% 
  filter(Country == "Italy") %>% 
  mutate(
    `5-MA` = slider::slide_dbl(
      Exports,
      mean,
      .before = 2,
      .after = 2,
      .complete = TRUE
      ),
    `9-MA` = slider::slide_dbl(
      Exports,
      mean,
      .before = 4,
      .after = 4,
      .complete = TRUE
      )
  ) %>% 
  autoplot(Exports) +
  geom_line(
    aes(
      y = `5-MA`,
      colour = "darkorange"
      ), 
    ) +
  geom_line(
    aes(
      y = `9-MA`,
      colour = "dodgerblue"
      ), 
    ) +
  labs(
    y = "% of GDP",
    title = "Total Australian exports",
    colour = "MA smoothings"
    ) +
  scale_color_discrete(
    labels = c(
      "5-MA", 
      "9-MA"
      )
    )
```

:::{.callout-important}
The beginning and end of MA trends are missing: this is caused by the fact that we cannot compute the average near the boundaries of our dataset.
:::

For seasonal data the choice of $m$ is fundamental:

```{r MA example 2}
vic_elec |>
  filter(year(Time) == 2014) |>
  index_by(Date = date(Time)) %>% 
summarise(
    Demand = sum(Demand) / 1e3
  ) %>%  mutate(
    `5-MA` = slider::slide_dbl(
      Demand,
      mean,
      .before = 2,
      .after = 2,
      .complete = TRUE
      ),
    `11-MA` = slider::slide_dbl(
      Demand,
      mean,
      .before = 5,
      .after = 5,
      .complete = TRUE
      )
  ) %>% 
  ggplot(
    aes(
      x = Date
    )
  ) +
  geom_line(
    aes(
      y = Demand
    )
  ) +
  geom_line(
    aes(
      y = `5-MA`,
      colour = "darkorange"
    )
  ) +
  geom_line(
    aes(
      y = `11-MA`,
      colour = "dodgerblue"
    )
  ) +
  labs(
    title = "Monthly electricity demand for 2014",
    colour = "MA smoothings"
  )  +
  scale_color_discrete(
    labels = c(
      "5-MA", 
      "9-MA"
      )
    )

```

To _smooth out the seasonal component_ we use moving averages of the order of the periodicity of the seasonality.

For example, in the case of _weekly data_, we can use a 7-MA. If we have _even_ numbers, we can take a _moving average of a moving average_: this allows centering of the data and keeping symmetry^[This works effectively as a _weighted_ moving average.].

:::{.callout-tip}
For seasonal data with even period $m$ take 2 × MA of period $m$.
:::

```{r nxm MA}
us_employment %>% 
     filter(
       year(Month) >= 1990, 
       Title == "Retail Trade"
       ) %>% 
     select(
       -Series_ID
       ) %>% 
     mutate(
       `12-MA` = slider::slide_dbl(
         Employed, 
         mean, 
         .before = 5, 
         .after = 6, 
         .complete = TRUE
         ),
       `2x12-MA` = slider::slide_dbl(
         `12-MA`, 
         mean, 
         .before = 1, 
         .after = 0, 
         .complete = TRUE
         )
       ) %>% 
     na.omit() %>%
     ggplot(
       aes(
         x = Month
       )
     ) +
  geom_line(
    aes(
      y = Employed, 
      ),
       color = "grey60"
    ) +
  geom_line(
    aes(
      y = `2x12-MA`, 
       color = "darkorange"
      )
    )+
  labs(
       y = "Persons (thousands)",
       title = "Total employment in US retail",
       color = ""
     ) +
  scale_color_discrete(
    labels = c(
      "2x12-MA"
      )
    )
```

# Classical Decomposition

To obtain this decomposition, the following steps are needed:

1. The trend-cycle component is estimated using the appropriate MA smoothing.

:::{.callout-tip}
If $m$ is even, use 2x $m$-NA; if $m$ is odd, use $m$-MA.
:::

:::{.callout-caution}
The main assumption is that the season pattern _is the same across time_.
:::

## Additive (classical) Decomposition

2. Derive the de-trended series:

$$
y_t - \hat{T}_t
$${#eq-detrendadd}

3a. Estimate the _seasonal components_ by taking the average of all detrended values for each for that season^[E.g.: take the average of all de-trended January values.].

3b. Correct the individual seasonal terms to ensure they sum to 0.

4. The remainder component is calculated by subtracting the estimated seasonal and trend-cycle components:

$$
\hat{R}_t = y_t - \hat{T}_t - \hat{S}_t
$${#eq-remainadd}

5. Derive a de-seasonalized series as $y_t - S_t$.

## Multiplicative (classical) Decomposition

2. Derive the de-trended series:

$$
\frac {y_t} {\hat{T}_t}
$${#eq-detrendmul}


3a. Estimate the seasonal components by taking the average of all the detrended values for each for that season^[E.g.: take the average of all de-trended January values.].

3b. Correct the individual seasonal terms to ensure they sum to $m$.

4. The remainder component is calculated by dividing out the estimated seasonal and trend-cycle components:

$$
\hat{R}_t = \frac{y_t}{\hat{T}_t \hat{S}_t}
$${#eq-remainmul}

```{r mult class decomposition example}
#|fig-height: 15
p1 <- aus_production %>% 
  filter(
    year(Quarter) >= 1970
  ) %>% 
  autoplot(Gas)
p2 <- aus_production %>% 
  filter(
    year(Quarter) >= 1970
  ) %>% 
 model(
       classical_decomposition(
         Gas,
         type = "multiplicative"
         )
       ) %>% 
     components() %>% 
     autoplot() +
  labs(
    x = "Year",
    title = "Classical additive decomposition \n AUS Gas production"
    ) 

p1 / p2
```

## Comments on Classical Decomposition

> While still used, classical decomposition is not recommended.

Its main drawbacks are:

- The estimate of the trend cycle (and therefore the remainder) is unavailable for the first few and last few observations; moreover, it tends to oversmooth rapid rises and falls in the data.
- The assumption of _constant seasonal terms_ might be too restrictive.
- The estimate is _not robust_: being a _mean_, it can be overly sensitive to large variations in a small number of points.

:::{.callout-note}
To overcome these drawbacks, common methods used by statistical institutes are STL decomposition and X-11/SEATS.
:::

We will focus on _exponential smoothing_ instead because it provides more general tools.

# Exponential smoothing

> Exponential smoothing is based on a simple concept: a weight is given to each observation; this weight decays _exponentially_, thus modifying the relative importance of the past values of a time series. 

The decay rate can be adjusted and this method can be adapted for data with and without a trend and/or seasonal pattern.

Forecasting _benchmark methods_ are represented by simple extremes:

1. The forecast is equal to the mean of the past observations: 

$$
y_{T + h \vert T} = \frac{1}{T} \sum y_i
$${#eq-meanbench}

2. The forecast is equal to the last observation:

$$
y_{T + h \vert T} = y_T
$${#eq-naivebench}

```{r create google stock tsibble}
#| echo: false
goog_stock <- gafa_stock %>% 
  mutate(
    trading_day = row_number()
    ) %>% 
  filter(
    Symbol == "GOOG",
    year(Date) >= 2018
    ) %>% 
  update_tsibble(
    index = trading_day, 
    regular = TRUE
    ) 
```

```{r benchmark forecasting google stock}
goog_stock %>%
  model(
    Mean = MEAN(Close),
    Naive = NAIVE(Close)
  )  %>% 
  forecast(
    h = 50
    )  %>% 
  autoplot(
    goog_stock, 
    level = NULL
    ) +
  labs(
    title = "Google closing stock price", 
    y = "$US"
    ) +
  guides(
    colour = guide_legend(
      title = "Forecast"
      )
    )
```

Exponential smoothing allows to achieve forecasts that are _in between_ these extremes, giving heavier weights to later observations.

Naming conventions are attributed to the most important variation of exponential smoothing:

1. Single, Double, Triple Exponential Smoothing.
2. Holt, Holt-Winter's Method.

## Simple Exponential Smoothing

> This method is appropriate for data with no trend and no seasonality.

The forecast equation is:

$$
\hat{y}_{T +1 \vert T} = \alpha y_T + \alpha (1 - \alpha) y_{T -1} + \alpha (1 - \alpha)^2 y_{T -2} + …
$$

Where $0 \leq \alpha \leq 1$, the _smoothing parameter_^[For $\alpha = 0$, we revert to the _mean method_. For $\alpha = 1$, instead, to the _naïve method_.].

```{r ses smoothing fit}
fit <- goog_stock %>% 
  model(
    "alpha = 0.1" = ETS(Close ~ error("A") + trend("N", alpha = .1) + season("N")),
    "alpha = 0.2" = ETS(Close ~ error("A") + trend("N", alpha = .5) + season("N")),
    "alpha = 0.7" = ETS(Close ~ error("A") + trend("N", alpha = .8) + season("N"))
    ) 
fc <- fit %>%
  augment()
```

```{r ses smoothing plot}
#| fig-height: 12
fc  %>% 
  autoplot(Close) +
  geom_line(
    aes(
      x = trading_day, 
      y = .fitted
      )
    ) +
  labs(
    y="$US", 
    title="Google closing stock price SES"
    ) +
  scale_color_brewer(
    type = "qual", 
    palette = "Dark2"
    )
```


### General Framework

> Forecast equation:

$$
\hat{y}_{t + h \vert t} = l_t
$${#eq-sesgenforecast}

> Smoothing equation:

$$
l_t = \alpha y_t + (1 - \alpha) l_{t-1}
$${#eq-sesgensmoothing}

- $l_t$ is the level (or smoothed value) of the series at time $t$.
- $\hat{y}_{t + 1 \vert t} = \alpha y_t + (1 - \alpha) \hat{y}_{t \vert t -1}$.

These two components can be iterated to get the exponentially weighted moving average form.

### Weighted Average Form

> Note that at time $t = 1$ we have to decide a value for $l_0$, because $\hat{y}_{1 \vert 0} = l_0$^[Data ends at that point.].

Iterating on @eq-sesgenforecast and @eq-sesgensmoothing we get the following form, for time $T + 1$:

$$
\hat{y}_{T + 1 \vert T} = \sum_{j = 0}^{T - 1} \alpha(1 - \alpha)^j y_{T - j} + (1 - \alpha)^T l_0
$${#eq-sesweightavgform}

The last term becomes very small for large $T$; the forecast consists of the _weighted average_ of all observations.

### Smoothing Parameters Choice

The parameters to be estimated are $\alpha$ and $l_0$. Similarly to regression, we can choose optimal parameters by minimizing the SSE^[Sum of Squared Errors.].

$$
SSE = \sum_{t  = 1}^T (y_t - \hat{y}_{t \vert t -1})^2
$${#eq-SSE}

There is not a closed-form solution: _numerical optimization_ is needed.

```{r weighted average fit}
fit <- goog_stock %>%  
  model(
    ETS(
      Close ~ error("A") + trend("N") + season("N"))
    ) 
fc <- fit %>%
  augment()
```

```{r weighted average parameters table}
tidy(fit)
```

```{r weighted average plot}
fc  %>% 
  autoplot(Close) +
  geom_line(
    aes(
      x = trading_day, 
      y = .fitted
      ),
    color = "limegreen"
    ) +
  labs(
    y="$US", 
    title="Google closing stock price SES"
    ) +
  scale_color_brewer(
    type = "qual", 
    palette = "Dark2"
    )
```

## Models with Trend

The principle is the same as SES: we can estimate a trend by giving more weight to more recent changes. There are two extreme situations:

- $y_T - y_{T - 1}$
- $y_T - y_1$

The average change is $\frac{y_T - y_1}{T}$. This method is called _Double Exponential Smoothing_ or _Holt's Method_.

The component form is:

> Forecast equation:

$$
\hat{y}_{t + h \vert t} = l_t + hb_t
$${#eq-desforecast}

> Level equation:

$$
l_t = \alpha y_t + (1 - \alpha)(l_{t - 1} + b_{t - 1})
$${#eq-deslevel}

> Trend component:

$$
b_t = \beta^* (l_t - l_{t - 1}) + (1 - \beta^*) b_{t - 1}
$${#eq-destrend}

- We have two smoothing parameters, $0 \leq \alpha, \beta^* \leq 1$. 
- The $l_t$ level is a weighted average between $y_t$ and a one-step ahead forecast for time $t$.
- The $b_t$ slope is the weighted average of $(l_t - l_{t -1})$ and $b_{t - 1}$, which are the _current_ and _previous_ estimate of the slope.

> $\alpha, \beta^*, l_0, b_0$ are chosen to minimize the SSE.


```{r des fit}
fit <- goog_stock %>% 
  model(
    "beta = 0.1" = ETS(
      Close ~ error("A") + trend("A", beta = 0.1) + season("N")
      ),
    "beta = 0.5" = ETS(
      Close ~ error("A") + trend("A", beta = 0.5) + season("N")
      ),
    "beta = 0.9" = ETS(
      Close ~ error("A") + trend("A", beta = 0.9) + season("N")
      )
  )
fc <- fit %>%
  augment()
```

```{r des plot}
fc  %>% 
  autoplot(Close) +
  geom_line(
    aes(
      x = trading_day, 
      y = .fitted
      )
    ) +
  labs(
    y="$US", 
    title="Google closing stock price SES"
    ) +
  scale_color_brewer(
    type = "qual", 
    palette = "Dark2"
    )
```

## Damped Trend Method

:::{.callout-warning}
In Holt's method, the linear trend is _constant into the future_: this can lead to over-forecast.
:::

This method is based on _dampening_ the trend to a _flat line_ sometime in the future. This is a popular and successful method; another parameter is introduced to control the damping, $\phi$.

> Forecast equation:

$$
\hat{y}_{t + h \vert t} = l_t + (\phi + \phi^2 + … + phi^h) b_t
$${#eq-dampforecast}

> Level equation:

$$
l_t = \alpha y_t + (1 - \alpha)(l_{t - 1} + \phi b_{t - 1})
$${#eq-damplevel}

> Trend component:

$$
b_t = \beta^* (l_t - l_{t - 1}) + (1 - \beta^*) \phi b_{t - 1}
$${#eq-damptrend}

- The damping parameter is $0 \leq \phi \leq 1$.
- If $\phi = 1$, this method becomes identical to Holt's linear trend.
- As $h \to \infty$, $\hat{y}_{T + h \vert T} \to l_T + \phi b_T / (1 - \phi)$

This method yields _trended_ short-run forecasts and _constant_ long-run forecasts.

```{r damp fit}
fit <- goog_stock %>% 
  filter(
    trading_day >= 4900
  ) %>% 
  model(
    "linear" = ETS(
      Close ~ error("A") + trend("A") + season("N")
      ),
    "damped" = ETS(
      Close ~ error("A") + trend("Ad") + season("N")
      )
  )
fc <- fit %>%
  forecast(
    h = 30
  )
```

```{r damp forecast}
fc %>% 
autoplot(
    goog_stock, 
    level = NULL
    ) +
  labs(
    title = "Google closing stock price", 
    y = "$US"
    ) +
  guides(
    colour = guide_legend(
      title = "Forecast"
      )
  )
```

## Models with Seasonality
 
 To deal with _seasonality_ we need to adapt the smoothing principle:

 > The _seasonal component_ has to be estimated giving _more weight_ to _more recent values of the seasonal components_.

 This approach is typically combined with a trended model; seasonality can enter the model either in an _additive_ (@eq-additive) or _multiplicative_ (@eq-multiplicative) way. It is called Triple Exponential Smoothing_ or the _Holt-Winters method_.

### Holt-Winters Additive Method

:::{.callout-tip}
This model is appropriate when seasonal variations are independent of the level of the series.
:::

 The component form is:

> Forecast equation:

$$
\hat{y}_{t + h \vert t} = l_t + hb_t + s_{t + h - m(k + 1)}
$${#eq-tesaddforecast}

> Level equation:

$$
l_t = \alpha(y_t - s_{t - m}) + (1 - \alpha)(l_{t - 1} + b_{t - 1})
$${#eq-tesaddlevel}

> Trend component:

$$
b_t = \beta^* (l_t - l_{t - 1} - b_{t - 1}) + (1 - \beta^*) b_{t - 1}
$${#eq-tesaddtrend}

> Seasonal component:

$$
s_t = \gamma(y_t - l_{t - 1} - b_{t - 1}) + (1 - \gamma) s_{t - m}
$${#eq-tesaddseason}

- $k$ is the integer part of $(h - 1)/m$. Ensures estimates from the final years are used for forecasting.
- $0 \leq \alpha, \beta^* \leq 1$.
- $0 \leq \gamma \leq 1 - \alpha$.
- $m$ is the period of seasonality.

:::{.callout-caution}
With additive methods, seasonality is expressed in _absolute terms_; within each year $\sum_i s_i \approx 0$.
:::


```{r aus beer data}
#| echo: false
aus_beer <- aus_production %>% 
  select(Beer) %>% 
  filter(
    year(Quarter) >= 2000
  )
```

```{r seasonal add fit}
fit <- aus_beer %>% 
  model(
    ETS(
      Beer ~ error("A") + trend("A") + season("A")
    )
  )

fc <- fit %>% 
  forecast(
    h = 10
  )
```

```{r seasonal add plot}
fc %>% 
autoplot(
    aus_beer, 
    level = NULL
    ) +
  labs(
    title = "AUS Beer production", 
    y = "Beer"
    ) +
  guides(
    colour = guide_legend(
      title = "Forecast"
      )
  )
```


### Holt-Winters Multiplicative Method

:::{.callout-tip}
This model is appropriate when seasonal variations are changing proportionally to the level of the series.
:::

The component form is:

> Forecast equation:

$$
\hat{y}_{t + h \vert t} = (l_t + hb_t) s_{t + h - m(k + 1)}
$${#eq-tesmulforecast}

> Level equation:

$$
l_t = \alpha \frac{y_t}{s_{t - m}}  + (1 - \alpha)(l_{t - 1} + b_{t - 1})
$${#eq-tesmullevel}

> Trend component:

$$
b_t = \beta^* (l_t - l_{t - 1} - b_{t - 1}) + (1 - \beta^*) b_{t - 1}
$${#eq-tesmultrend}

> Seasonal component:

$$
s_t = \gamma \frac{y_t}{ l_{t - 1} - b_{t - 1}} + (1 - \gamma) s_{t - m}
$${#eq-tesmulseason}

- $k$ is the integer part of $(h - 1)/m$. Ensures estimates from the final years are used for forecasting.
- $0 \leq \alpha, \beta^* \leq 1$.
- $0 \leq \gamma \leq 1 - \alpha$.
- $m$ is the period of seasonality.

:::{.callout-caution}
With multiplicative methods, seasonality is expressed in _relative terms_; within each year $\sum_i s_i \approx m$.
:::

```{r aus gas prod data}
#| echo: false
aus_gas <- aus_production %>% 
  select(Gas)
```

```{r seasonal mult fit}
fit <- aus_gas %>% 
  model(
    additive = ETS(
      Gas ~ error("A") + trend("A") + season("A")
    ),
    multiplicative = ETS(
      Gas ~ error("A") + trend("A") + season("M")
    )
  )

fc <- fit %>% 
  forecast(
    h = 30
  )
```

```{r seasonal mult plot}
fc %>% 
autoplot(
    aus_gas, 
    level = NULL
    ) +
  labs(
    title = "AUS Gas production", 
    y = "Gas"
    ) +
  guides(
    colour = guide_legend(
      title = "Forecast"
      )
  ) +
  scale_color_brewer(
    type = "qual", 
    palette = "Dark2"
    )
```

### Holt-Winters Damped Method

This approach combines Holt's damped method with the seasonality of the Holt-Winters method.

The component form is:

> Forecast equation:

$$
\hat{y}_{t + h \vert t} = [l_t + (\phi * \phi^2 + ... + \phi^h) b_t ]s_{t + h - m(k + 1)}
$${#eq-tesmuldampforecast}

> Level equation:

$$
l_t = \alpha \frac{y_t}{s_{t - m}}  + (1 - \alpha)(l_{t - 1} + \phi b_{t - 1})
$${#eq-tesmuldamplevel}

> Trend component:

$$
b_t = \beta^* (l_t - l_{t - 1}) + (1 - \beta^*) \phi b_{t - 1}
$${#eq-tesmuldamptrend}

> Seasonal component:

$$
s_t = \gamma \frac{y_t}{ l_{t - 1} - \phi b_{t - 1}} + (1 - \gamma) s_{t - m}
$${#eq-tesmuldampseason}

- $k$ is the integer part of $(h - 1)/m$. Ensures estimates from the final years are used for forecasting.
- $0 \leq \alpha, \beta^* \leq 1$.
- $0 \leq \gamma \leq 1 - \alpha$.
- $m$ is the period of seasonality.

```{r seasonal damp fit}
fit <- aus_gas %>% 
  model(
    "multiplicative+damped" = ETS(
      Gas ~ error("A") + trend("Ad") + season("M")
    ),
    multiplicative = ETS(
      Gas ~ error("A") + trend("A") + season("M")
    )
  )

fc <- fit %>% 
  forecast(
    h = 50
  )
```

```{r seasonal damp plot}
fc %>% 
autoplot(
    aus_gas, 
    level = NULL
    ) +
  labs(
    title = "AUS Gas production", 
    y = "Gas"
    ) +
  guides(
    colour = guide_legend(
      title = "Forecast"
      )
  ) +
  scale_color_brewer(
    type = "qual", 
    palette = "Dark2"
    )
```

